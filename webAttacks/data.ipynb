{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original columns: ['Unnamed: 0', 'Method', 'User-Agent', 'Pragma', 'Cache-Control', 'Accept', 'Accept-encoding', 'Accept-charset', 'language', 'host', 'cookie', 'content-type', 'connection', 'lenght', 'content', 'classification', 'URL']\n",
      "Dropped columns with >70.0% missing values: ['content-type', 'lenght', 'content']\n",
      "Warning: No 'length'-like column found, created with default 0.\n",
      "Warning: No content-like column (content, payload, body) found, skipping decoding.\n",
      "Cleaned dataset saved. Original rows: 61065, After cleaning: 61065\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from urllib.parse import urlparse, unquote\n",
    "import re\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('data/csic_database.csv')\n",
    "\n",
    "# Print columns for debugging\n",
    "print(\"Original columns:\", df.columns.tolist())\n",
    "\n",
    "# Step 0: Calculate missing value ratio and drop columns > 70% missing (do this first)\n",
    "missing_ratio = df.isna().mean()\n",
    "threshold = 0.7\n",
    "columns_to_drop = missing_ratio[missing_ratio > threshold].index\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "print(f\"Dropped columns with >{threshold*100}% missing values: {list(columns_to_drop)}\")\n",
    "\n",
    "# Step 1: Remove Unnamed Column\n",
    "df = df.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "\n",
    "# Step 2: Determine normal host (after ensuring 'URL' exists)\n",
    "def get_host(url):\n",
    "    return urlparse(str(url)).netloc\n",
    "if 'URL' in df.columns:\n",
    "    normal_host = df['URL'].apply(get_host).mode()[0]\n",
    "else:\n",
    "    normal_host = 'localhost:8080'  # Fallback if 'URL' missing\n",
    "    print(\"Warning: 'URL' column not found, using fallback host.\")\n",
    "\n",
    "# Step 3: Fix Spelling (check for 'contentLength' or 'lenght')\n",
    "if 'lenght' in df.columns:\n",
    "    df = df.rename(columns={'lenght': 'length'})\n",
    "elif 'contentLength' in df.columns:\n",
    "    df = df.rename(columns={'contentLength': 'length'})\n",
    "else:\n",
    "    df['length'] = 0  # Create if missing\n",
    "    print(\"Warning: No 'length'-like column found, created with default 0.\")\n",
    "\n",
    "# Step 4: Standardize 'connection' (only if it exists)\n",
    "if 'connection' in df.columns:\n",
    "    df['connection'] = df['connection'].str.replace('Connection: close', 'close', case=False).str.strip()\n",
    "else:\n",
    "    print(\"Warning: 'connection' column not found, skipping standardization.\")\n",
    "\n",
    "# Step 5: Decode encoded data (URL and content-like column if present)\n",
    "if 'URL' in df.columns:\n",
    "    df['URL'] = df['URL'].apply(lambda x: unquote(str(x)) if pd.notna(x) else x)\n",
    "content_col = next((col for col in ['content', 'payload', 'body'] if col in df.columns), None)\n",
    "if content_col:\n",
    "    df[content_col] = df[content_col].apply(lambda x: unquote(str(x)) if pd.notna(x) else x)\n",
    "else:\n",
    "    print(\"Warning: No content-like column (content, payload, body) found, skipping decoding.\")\n",
    "\n",
    "# Step 6: Flag anomalies\n",
    "def categorize_anomaly(row, normal_host):\n",
    "    label_col = 'label' if 'label' in df.columns else 'classification'\n",
    "    if label_col in df.columns and row[label_col] in [0, 'Normal']:\n",
    "        return 'None'\n",
    "    url = str(row['URL']).lower() if 'URL' in df.columns else ''\n",
    "    host = get_host(row['URL']) if 'URL' in df.columns else normal_host\n",
    "    content = str(row[content_col]).lower() if content_col else ''\n",
    "    if '<script' in content or '<script' in url:\n",
    "        return 'XSS'\n",
    "    elif \"'\" in content or '\"' in content or 'union' in content or 'select' in content:\n",
    "        return 'SQLi'\n",
    "    elif '.bak' in url or '.inc' in url or '..' in url:\n",
    "        return 'Path Traversal'\n",
    "    elif row.get('Method', '') == 'PUT' or host != normal_host:\n",
    "        return 'Suspicious Method/Host'\n",
    "    return 'Other'\n",
    "\n",
    "df['anomaly_type'] = df.apply(categorize_anomaly, axis=1, args=(normal_host,))\n",
    "\n",
    "# Step 7: Normalize remaining missing values\n",
    "df = df.fillna('')\n",
    "\n",
    "# Step 8: Remove duplicates\n",
    "df = df.drop_duplicates(keep='first')\n",
    "\n",
    "# Step 9: Validate data types\n",
    "df['length'] = pd.to_numeric(df['length'], errors='coerce').fillna(0).astype(int)\n",
    "label_col = 'label' if 'label' in df.columns else 'classification'\n",
    "if label_col in df.columns:\n",
    "    df[label_col] = df[label_col].apply(lambda x: 0 if x in ['Normal', 0] else 1)\n",
    "else:\n",
    "    df['label'] = 0  # Create if missing\n",
    "    print(\"Warning: No label/classification column found, created with default 0.\")\n",
    "\n",
    "# Step 10: Address outliers\n",
    "df['outlier_flag'] = df.apply(\n",
    "    lambda row: 'Yes' if ('URL' in df.columns and get_host(row['URL']) != normal_host) or row.get('Method', '') == 'PUT' else 'No',\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Save\n",
    "df.to_csv('data/fully_cleaned_dataset.csv', index=False)\n",
    "print(f\"Cleaned dataset saved. Original rows: {len(pd.read_csv('data/csic_database.csv'))}, After cleaning: {len(df)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc3e32e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9da8bcd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_new_data(test_data_path, reference_data_path, model_features, scaler_path, output_transformed_path):\n",
    "    # Load the reference data (training dataset) and the test data (new dataset)\n",
    "    reference_data = pd.read_csv(reference_data_path)\n",
    "    test_data = pd.read_csv(test_data_path)\n",
    "    \n",
    "    # Step 1: Ensure all the required features are present in the test data\n",
    "    missing_cols = set(model_features) - set(test_data.columns)\n",
    "    for col in missing_cols:\n",
    "        # If a feature is missing, sample a value from the reference data (training data)\n",
    "        sampled_value = reference_data[col].dropna().sample(1).values[0]\n",
    "        test_data[col] = sampled_value\n",
    "\n",
    "    # Step 2: Reorder the columns to match the training feature order\n",
    "    test_data = test_data[model_features]\n",
    "    \n",
    "    # Step 3: Handle missing values in the test data for features that exist\n",
    "    for col in model_features:\n",
    "        if col in test_data.columns and test_data[col].isnull().sum() > 0:\n",
    "            # Sample from the reference data if values are missing, otherwise fill with 0\n",
    "            sampled_values = reference_data[col].dropna()\n",
    "            if not sampled_values.empty:\n",
    "                test_data[col] = test_data[col].apply(lambda x: random.choice(sampled_values) if pd.isna(x) else x)\n",
    "            else:\n",
    "                test_data[col] = test_data[col].fillna(0)  # Fill with 0 if no sampled values\n",
    "\n",
    "    # Step 4: Handle inf and NaN values in the dataset\n",
    "    test_data.replace([np.inf, -np.inf], np.nan, inplace=True)  # Replace infinity values with NaN\n",
    "    test_data.fillna(test_data.mean(), inplace=True)  # Fill NaNs with the mean of the column\n",
    "    test_data = test_data.clip(upper=1e10)  # Clip extreme values to prevent outliers\n",
    "    test_data = test_data.astype(np.float32)  # Convert to appropriate type for processing\n",
    "\n",
    "    # Step 5: Load the pre-trained scaler and scale the data\n",
    "    scaler = joblib.load(scaler_path)  # Load the MinMaxScaler from file\n",
    "    test_data_scaled = scaler.transform(test_data)  # Apply scaling to the test data\n",
    "    \n",
    "    # Step 6: Save the transformed (scaled) test data\n",
    "    transformed_df = pd.DataFrame(test_data_scaled, columns=model_features)\n",
    "    transformed_df.to_csv(output_transformed_path, index=False)\n",
    "\n",
    "    print(f\"âœ… Transformed data saved to {output_transformed_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "970f48d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Bwd Seg Size Avg\n- ECE Flag Cnt\n- Fwd IAT Mean\n- Fwd IAT Min\n- Fwd IAT Tot\n- ...\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 20\u001b[0m\n\u001b[0;32m      2\u001b[0m model_features \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDst Port\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProtocol\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFlow Duration\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTot Fwd Pkts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTot Bwd Pkts\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotLen Fwd Pkts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTotLen Bwd Pkts\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFwd Pkt Len Max\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFwd Pkt Len Min\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIdle Std\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     17\u001b[0m ]\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Call the function with paths to your data\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m transform_new_data(\n\u001b[0;32m     21\u001b[0m     test_data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4th semester\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbenign_ftp_bruteforce.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# New data you want to process\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     reference_data_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4th semester\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbalanced_FTP-BruteForce.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Reference training data used to train the model\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     model_features\u001b[38;5;241m=\u001b[39mmodel_features,  \u001b[38;5;66;03m# List of features the model expects\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     scaler_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4th semester\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mModels\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mmin_max_scaler.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Pre-trained scaler for consistent scaling\u001b[39;00m\n\u001b[0;32m     25\u001b[0m     output_transformed_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124m4th semester\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mSE\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mproject\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDatasets\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtransformed_nigger.csv\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Output file for transformed data\u001b[39;00m\n\u001b[0;32m     26\u001b[0m )\n",
      "Cell \u001b[1;32mIn[14], line 34\u001b[0m, in \u001b[0;36mtransform_new_data\u001b[1;34m(test_data_path, reference_data_path, model_features, scaler_path, output_transformed_path)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Step 5: Load the pre-trained scaler and scale the data\u001b[39;00m\n\u001b[0;32m     33\u001b[0m scaler \u001b[38;5;241m=\u001b[39m joblib\u001b[38;5;241m.\u001b[39mload(scaler_path)  \u001b[38;5;66;03m# Load the MinMaxScaler from file\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m test_data_scaled \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform(test_data)  \u001b[38;5;66;03m# Apply scaling to the test data\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Step 6: Save the transformed (scaled) test data\u001b[39;00m\n\u001b[0;32m     37\u001b[0m transformed_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(test_data_scaled, columns\u001b[38;5;241m=\u001b[39mmodel_features)\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[1;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 140\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m f(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m    145\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[0;32m    146\u001b[0m         )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:508\u001b[0m, in \u001b[0;36mMinMaxScaler.transform\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Scale features of X according to feature_range.\u001b[39;00m\n\u001b[0;32m    495\u001b[0m \n\u001b[0;32m    496\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    504\u001b[0m \u001b[38;5;124;03m    Transformed data.\u001b[39;00m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    506\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 508\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    509\u001b[0m     X,\n\u001b[0;32m    510\u001b[0m     copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy,\n\u001b[0;32m    511\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mFLOAT_DTYPES,\n\u001b[0;32m    512\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow-nan\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    513\u001b[0m     reset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m )\n\u001b[0;32m    516\u001b[0m X \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_\n\u001b[0;32m    517\u001b[0m X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[0;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[0;32m    490\u001b[0m ):\n\u001b[0;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \n\u001b[0;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_feature_names(X, reset\u001b[38;5;241m=\u001b[39mreset)\n\u001b[0;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    554\u001b[0m         )\n",
      "File \u001b[1;32md:\\anaconda\\Lib\\site-packages\\sklearn\\base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[0;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    479\u001b[0m     )\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[1;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names seen at fit time, yet now missing:\n- Bwd Seg Size Avg\n- ECE Flag Cnt\n- Fwd IAT Mean\n- Fwd IAT Min\n- Fwd IAT Tot\n- ...\n"
     ]
    }
   ],
   "source": [
    "# Define the features expected by the model (same as used during training)\n",
    "model_features = [\n",
    "    'Dst Port', 'Protocol', 'Timestamp', 'Flow Duration', 'Tot Fwd Pkts', 'Tot Bwd Pkts',\n",
    "    'TotLen Fwd Pkts', 'TotLen Bwd Pkts', 'Fwd Pkt Len Max', 'Fwd Pkt Len Min',\n",
    "    'Fwd Pkt Len Mean', 'Fwd Pkt Len Std', 'Bwd Pkt Len Max', 'Bwd Pkt Len Min',\n",
    "    'Bwd Pkt Len Mean', 'Bwd Pkt Len Std', 'Flow Byts/s', 'Flow Pkts/s',\n",
    "    'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Std',\n",
    "    'Fwd IAT Max', 'Bwd IAT Tot', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
    "    'Bwd IAT Min', 'Fwd PSH Flags', 'Bwd PSH Flags', 'Fwd URG Flags', 'Bwd URG Flags',\n",
    "    'Fwd Header Len', 'Bwd Header Len', 'Fwd Pkts/s', 'Bwd Pkts/s', 'Pkt Len Mean',\n",
    "    'Pkt Len Std', 'Pkt Len Var', 'FIN Flag Cnt', 'RST Flag Cnt', 'PSH Flag Cnt',\n",
    "    'ACK Flag Cnt', 'URG Flag Cnt', 'CWE Flag Count', 'Down/Up Ratio', 'Fwd Byts/b Avg',\n",
    "    'Fwd Pkts/b Avg', 'Fwd Blk Rate Avg', 'Bwd Byts/b Avg', 'Bwd Pkts/b Avg',\n",
    "    'Bwd Blk Rate Avg', 'Init Fwd Win Byts', 'Init Bwd Win Byts', 'Fwd Act Data Pkts',\n",
    "    'Fwd Seg Size Min', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
    "    'Idle Std'\n",
    "]\n",
    "\n",
    "# Call the function with paths to your data\n",
    "transform_new_data(\n",
    "    test_data_path=r\"D:\\4th semester\\SE\\project\\Datasets\\benign_ftp_bruteforce.csv\",  # New data you want to process\n",
    "    reference_data_path=r\"D:\\4th semester\\SE\\project\\Datasets\\balanced_FTP-BruteForce.csv\",  # Reference training data used to train the model\n",
    "    model_features=model_features,  # List of features the model expects\n",
    "    scaler_path=r'D:\\4th semester\\SE\\project\\Models\\min_max_scaler.pkl',  # Pre-trained scaler for consistent scaling\n",
    "    output_transformed_path=r'D:\\4th semester\\SE\\project\\Datasets\\transformed_nigger.csv'  # Output file for transformed data\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
